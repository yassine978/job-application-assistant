name: Scheduled Job Scraping

on:
  # schedule:
  #   # Run every day at 9 AM UTC (10 AM Paris time)
  #   # DISABLED: Enable after configuring GitHub Secrets
  #   - cron: '0 9 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape-jobs:
    name: Scrape Job Listings
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright
      run: playwright install chromium

    - name: Run job scraper
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
        ADZUNA_API_KEY: ${{ secrets.ADZUNA_API_KEY }}
        DEBUG: False
      run: |
        python -c "
        from scrapers.scraper_factory import scraper_factory
        from database.db_manager import db_manager

        print('[*] Starting scheduled job scraping...')

        # Initialize database
        db_manager.initialize()

        # Scrape jobs
        jobs = scraper_factory.scrape_all_sources(
            keywords=['Python', 'Developer', 'Data Science', 'Machine Learning'],
            locations=['Paris', 'Remote'],
            max_age_days=7,
            auto_save_db=True
        )

        print(f'[OK] Scraped {len(jobs)} jobs')
        print(f'[OK] Jobs saved to database')
        "

    - name: Generate scraping report
      run: |
        echo "# Scraping Report" > scraping_report.md
        echo "Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> scraping_report.md
        echo "Status: Completed" >> scraping_report.md
        cat scraping_report.md

    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: scraping-report-${{ github.run_number }}
        path: scraping_report.md

  cleanup-old-jobs:
    name: Cleanup Old Jobs
    runs-on: ubuntu-latest
    needs: scrape-jobs

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Cleanup old jobs
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        python -c "
        from database.db_manager import db_manager
        from datetime import datetime, timedelta

        print('[*] Cleaning up old jobs...')

        # Initialize database
        db_manager.initialize()

        # Delete jobs older than 30 days
        cutoff_date = datetime.now() - timedelta(days=30)

        with db_manager.db as session:
            from database.models import Job
            deleted = session.query(Job).filter(
                Job.posting_date < cutoff_date
            ).delete()
            session.commit()

        print(f'[OK] Deleted {deleted} old jobs')
        "
      continue-on-error: true
