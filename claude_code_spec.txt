# AI-Powered Job Application Assistant - Technical Specification for Development

## PROJECT OVERVIEW

### System Description
An intelligent job application assistant powered by RAG (Retrieval-Augmented Generation) that:
1. Scrapes job postings from multiple sources
2. Stores user profiles (including optional projects) and application history in vector database
3. Filters and ranks jobs using RAG-enhanced algorithms
4. Generates contextually-aware, personalized CVs and cover letters
5. Intelligently selects and formats relevant projects for each job
6. Allows users to control CV length (1-2 pages) with smart content prioritization
7. Learns from past applications to improve recommendations
8. Stores data in PostgreSQL + Chroma vector database
9. Provides Streamlit dashboard with persistent sessions
10. **Does NOT auto-apply** - prepares materials for manual application

### Key Features
- **RAG Architecture**: Persistent memory using vector embeddings
- **Project Intelligence**: Parses README.md files and selects relevant projects per job
- **Page Limit Control**: Enforces 1 or 2 page CVs with content optimization
- **Semantic Matching**: Beyond keyword matching - understands context
- **Continuous Learning**: Improves recommendations over time

### Target Users
Job seekers (students, early-career professionals) seeking internships, alternance, or entry-level positions in France (French/English markets).

---

## TECHNOLOGY STACK

### Core Technologies
- **Language**: Python 3.9+
- **Framework**: Modular Python application
- **Version Control**: Git + GitHub

### AI/ML Stack
- **LLM Framework**: LangChain
- **LLM Provider**: Groq API (free tier) with Llama 3.1 8B model
  - Free: 14,400 requests/day, Rate limit: 30 requests/minute
- **RAG Architecture**: LangChain RAG pipeline
- **Vector Database**: Chroma (local persistence)
- **Embeddings**: HuggingFace `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)
- **Matching Algorithm**: scikit-learn (TF-IDF, cosine similarity)

### Additional Libraries
- **Markdown Parser**: `markdown` or `mistune`
- **File Processing**: `python-frontmatter`
- **PDF Generation**: `weasyprint`, `pypdf2` (with page control)

### Data Sources
- **Primary**: Welcome to the Jungle (Playwright scraping)
- **Secondary**: Adzuna API (free tier: 5,000 calls/month)
- **NOT using**: Direct LinkedIn scraping (ToS violation)

### Database
- **Primary**: PostgreSQL (structured data)
- **Vector DB**: Chroma (embeddings and vector search)
- **ORM**: SQLAlchemy
- **Migrations**: Alembic
- **Hosting**: Supabase (500MB free) or ElephantSQL (20MB free)

### Web Scraping
- **Tool**: Playwright (headless browser)
- **API Client**: requests library

### Data Processing
- **pandas**: Data manipulation
- **scikit-learn**: TF-IDF vectorization, cosine similarity

### Output & Interface
- **Export**: pandas (CSV), openpyxl (Excel)
- **Dashboard**: Streamlit with persistent sessions
- **Deployment**: Streamlit Cloud (free)

### CI/CD & DevOps
- **CI/CD**: GitHub Actions
- **Containerization**: Docker
- **Testing**: pytest
- **Code Quality**: flake8, black, mypy, bandit
- **Coverage**: pytest-cov + Codecov
- **Monitoring**: Sentry (free tier)

### Key Python Libraries
```
# Core
langchain-groq
langchain-chroma
langchain-community
playwright
pandas
python-dotenv
requests
streamlit
openpyxl
scikit-learn
psycopg2-binary
sqlalchemy
alembic

# RAG & Vector Store
chromadb
sentence-transformers
langchain-huggingface

# Markdown & File Processing
markdown
mistune
python-frontmatter

# PDF Generation
weasyprint
pypdf2

# Testing & Quality
pytest
pytest-cov
flake8
black
mypy
bandit
```

---

## RAG ARCHITECTURE

### RAG Architecture Diagram

```
┌─────────────────────────────────────────────────────────────┐
│                    USER INTERACTION LAYER                   │
│   (Streamlit Dashboard + Profile + Projects + CV Settings) │
└──────────────────────┬──────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────┐
│                   EMBEDDING LAYER                           │
│  HuggingFace Sentence Transformers (all-MiniLM-L6-v2)      │
│  Converts: Profile + Projects + Jobs → 384-dim vectors     │
└──────────────────────┬──────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────┐
│              VECTOR DATABASE (Chroma)                       │
│                                                             │
│  Collections:                                               │
│  ├── user_profiles (name, skills, experience)              │
│  ├── user_projects (title, description, tech stack) ★NEW★  │
│  ├── past_cvs (with project selection metadata) ★UPDATED★  │
│  ├── past_cover_letters                                    │
│  ├── job_descriptions                                      │
│  └── successful_applications                               │
│                                                             │
│  Features:                                                  │
│  • Project-to-job similarity matching ★NEW★                │
│  • Content prioritization for page limits ★NEW★            │
│  • Semantic search across all entities                     │
└──────────────────────┬──────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────┐
│          INTELLIGENT RAG PIPELINE ★ENHANCED★               │
│                                                             │
│  Step 1: RETRIEVE                                           │
│  ├── User profile                                          │
│  ├── Most relevant projects for this job (top 2-3) ★NEW★  │
│  ├── Past CVs for similar roles                           │
│  └── Successful applications                               │
│                                                             │
│  Step 2: PRIORITIZE CONTENT ★NEW★                         │
│  ├── Analyze page limit requirement (1 or 2 pages)        │
│  ├── Score each section by relevance                       │
│  ├── Select best projects that fit job description        │
│  ├── Determine which experiences to emphasize              │
│  └── Calculate optimal content distribution                │
│                                                             │
│  Step 3: AUGMENT                                            │
│  ├── Combine retrieved context with job description        │
│  ├── Include selected projects with relevance scores       │
│  ├── Add page limit instructions to prompt                 │
│  └── Build enriched prompt for LLM                         │
│                                                             │
│  Step 4: GENERATE                                           │
│  ├── Generate CV with smart content prioritization        │
│  ├── Format projects section (if user opted in)           │
│  ├── Ensure content fits within page limit                │
│  └── Maintain quality and completeness                     │
│                                                             │
│  Step 5: STORE                                              │
│  ├── Save generated document with metadata                 │
│  ├── Store which projects were selected                    │
│  ├── Embed for future retrieval                           │
│  └── Update learning patterns                              │
└──────────────────────┬──────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────┐
│              POSTGRESQL DATABASE                            │
│  Jobs, Users, Applications, Projects ★UPDATED★             │
└──────────────────────┬──────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────┐
│                   OUTPUT LAYER                              │
│  PDF (1-2 pages) + CSV/Excel + Dashboard                   │
└─────────────────────────────────────────────────────────────┘
```

### RAG Benefits

1. **User Profile Persistence**: Fill once, never re-enter
2. **Context-Aware Generation**: Uses relevant past documents
3. **Consistent Quality**: Same terminology across applications
4. **Intelligent Recommendations**: Based on application history
5. **Continuous Learning**: Improves over time
6. **Intelligent Project Selection**: Auto-selects relevant projects
7. **Page Length Management**: Respects 1-2 page constraints

---

## USER INPUTS

### First-Time User (Profile Setup)

#### 1. Personal Information
- Full name, Email, Phone
- LinkedIn URL (optional)
- Location preference

#### 2. Skills (list)
Example: ["Python", "TensorFlow", "React", "SQL", "Git"]

#### 3. Experience (list of dicts)
```json
{
  "title": "Data Science Intern",
  "company": "Tech Innovations",
  "duration": "6 months (Jan 2024 - Jun 2024)",
  "description": "Developed ML models...",
  "skills_used": ["Python", "scikit-learn", "SQL"]
}
```

#### 4. Education (list of dicts)
```json
{
  "degree": "Master in AI",
  "institution": "University of Paris",
  "year": 2025,
  "field": "Computer Science",
  "coursework": ["ML", "DL", "NLP"]
}
```

#### 5. Languages (list)
Example: ["French (Native)", "English (Fluent)"]

#### 6. **Projects (OPTIONAL - NEW)**

**Option A: User has projects**
```json
{
  "include_projects": true,
  "projects": [
    {
      "title": "E-Commerce Recommendation System",
      "readme_file": "project1_readme.md",
      "github_url": "https://github.com/user/project1",
      "demo_url": "https://demo.project1.com",
      "technologies": ["Python", "TensorFlow", "Flask"],
      "highlights": [
        "Built ML engine serving 10k users",
        "Improved CTR by 35%"
      ]
    }
  ]
}
```

**How Projects Are Processed**:
1. User uploads `.md` file (README from GitHub)
2. System parses markdown:
   - Extract title
   - Extract description (first paragraph)
   - Extract technologies (badges, keywords)
   - Extract highlights (bullet points, achievements)
3. System embeds entire project content
4. Stored in PostgreSQL + Chroma

**Option B: No projects**
```json
{
  "include_projects": false,
  "projects": []
}
```

#### 7. **CV Preferences (NEW)**
```json
{
  "cv_length": 1,  // 1 or 2 pages
  "include_projects_in_cv": true,
  "max_projects_per_cv": 3,  // Default: 3, Max: 5
  "project_detail_level": "concise"  // "concise" or "detailed"
}
```

**CV Length Guidelines**:
- **1 page**: Entry-level, internships (industry standard)
- **2 pages**: Multiple years of experience

**Content Prioritization (1-page)**:
- Contact (always)
- Summary (2-3 lines)
- Skills (top 8-10)
- Experience (2-3 roles, condensed)
- Projects (top 2-3, 2-3 lines each)
- Education (condensed)
- Languages (one line)

**Content Prioritization (2-page)**:
- All sections expanded
- More projects (up to 5)
- More detailed descriptions

#### 8. Preferences
- Preferred job types
- Preferred locations
- Remote work preference
- Salary expectations (optional)

### Returning User (Job Search)
```json
{
  "keywords": ["AI Engineer", "ML"],
  "location": "Paris",  // Auto-filled
  "job_types": ["Internship"],  // Auto-filled
  "num_top_jobs": 15,
  "max_age_days": 7
}
```

---

## DATABASE SCHEMA

### PostgreSQL Tables

```sql
-- Users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(255) NOT NULL,
    phone VARCHAR(50),
    linkedin_url VARCHAR(255),
    location_preference VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP,
    preferences JSONB
);

-- User profiles table
CREATE TABLE user_profiles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    skills TEXT[],
    experience JSONB,
    education JSONB,
    languages TEXT[],
    chroma_id VARCHAR(255),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ★ NEW: User projects table
CREATE TABLE user_projects (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    readme_content TEXT,
    github_url VARCHAR(500),
    demo_url VARCHAR(500),
    technologies TEXT[],
    highlights TEXT[],
    chroma_id VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_projects (user_id),
    INDEX idx_project_title (title)
);

-- ★ NEW: CV preferences table
CREATE TABLE cv_preferences (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE UNIQUE,
    cv_length INTEGER DEFAULT 1,
    include_projects BOOLEAN DEFAULT FALSE,
    max_projects_per_cv INTEGER DEFAULT 3,
    project_detail_level VARCHAR(20) DEFAULT 'concise',
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Jobs table
CREATE TABLE jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source VARCHAR(50) NOT NULL,
    job_title VARCHAR(255) NOT NULL,
    company_name VARCHAR(255) NOT NULL,
    location VARCHAR(255),
    job_type VARCHAR(50),
    description TEXT,
    required_skills TEXT[],
    posting_date DATE,
    application_url TEXT,
    salary_range VARCHAR(100),
    language VARCHAR(10),
    chroma_id VARCHAR(255),
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_company_title (company_name, job_title),
    INDEX idx_posting_date (posting_date),
    INDEX idx_job_type (job_type)
);

-- Applications table
CREATE TABLE applications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES jobs(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    match_score FLOAT NOT NULL,
    match_score_breakdown JSONB,
    status VARCHAR(50) DEFAULT 'pending',
    applied_date TIMESTAMP,
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_match (user_id, match_score DESC),
    INDEX idx_status (status)
);

-- ★ UPDATED: Documents table with project metadata
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    application_id UUID REFERENCES applications(id) ON DELETE CASCADE,
    document_type VARCHAR(20) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    language VARCHAR(10),
    cv_length INTEGER,  -- ★ NEW
    projects_included UUID[],  -- ★ NEW
    projects_metadata JSONB,  -- ★ NEW
    chroma_id VARCHAR(255),
    generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    generation_time_seconds FLOAT,
    ai_model_used VARCHAR(100),
    word_count INTEGER,
    INDEX idx_application_type (application_id, document_type)
);

-- ★ NEW: Project usage tracking
CREATE TABLE project_usage_stats (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID REFERENCES user_projects(id) ON DELETE CASCADE,
    job_id UUID REFERENCES jobs(id) ON DELETE CASCADE,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    relevance_score FLOAT,
    selected BOOLEAN,
    selection_reason TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_project_usage (project_id),
    INDEX idx_project_selection (project_id, selected)
);

-- User sessions table
CREATE TABLE user_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    session_start TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    session_end TIMESTAMP,
    actions JSONB,
    searches_performed INTEGER DEFAULT 0,
    applications_generated INTEGER DEFAULT 0
);

-- Scraping logs table
CREATE TABLE scraping_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source VARCHAR(50) NOT NULL,
    jobs_scraped INTEGER DEFAULT 0,
    successful BOOLEAN DEFAULT TRUE,
    error_message TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    duration_seconds FLOAT
);
```

### Chroma Collections

```python
# Collection 1: User Profiles
chroma_client.create_collection(
    name="user_profiles",
    metadata={"description": "User profiles", "embedding_dimension": 384}
)

# Collection 2: User Projects ★NEW★
chroma_client.create_collection(
    name="user_projects",
    metadata={"description": "User projects with README", "embedding_dimension": 384}
)

# Collection 3: Job Descriptions
chroma_client.create_collection(
    name="job_descriptions",
    metadata={"description": "Job postings", "embedding_dimension": 384}
)

# Collection 4: Generated CVs
chroma_client.create_collection(
    name="past_cvs",
    metadata={"description": "Past CVs", "embedding_dimension": 384}
)

# Collection 5: Cover Letters
chroma_client.create_collection(
    name="past_cover_letters",
    metadata={"description": "Cover letters", "embedding_dimension": 384}
)

# Collection 6: Successful Applications
chroma_client.create_collection(
    name="successful_applications",
    metadata={"description": "High-scoring jobs", "embedding_dimension": 384}
)
```

---

## COMPLETE SYSTEM WORKFLOW

### Stage 1: User Onboarding

**Step 1.1: User Registration**
- Email + password authentication
- User ID generated

**Step 1.2: Profile Creation**
- User fills profile form
- Validate and sanitize data
- Store in PostgreSQL

**Step 1.3: ★ Projects Setup (OPTIONAL)**
```python
def handle_project_upload(user_id, project_data, readme_file):
    # 1. Read .md file
    readme_content = readme_file.read().decode('utf-8')
    
    # 2. Parse README
    project_info = parse_project_readme(readme_content)
    # Extracts: title, description, technologies, highlights
    
    # 3. Create project record
    project = {
        "user_id": user_id,
        "title": project_data.get('title') or project_info['title'],
        "description": project_info['description'],
        "readme_content": readme_content,
        "github_url": project_data.get('github_url'),
        "technologies": project_info['technologies'],
        "highlights": project_info['highlights']
    }
    
    # 4. Save to PostgreSQL
    project_id = db_manager.save_project(project)
    
    # 5. Embed project
    project_text = f"""
    Title: {project['title']}
    Description: {project['description']}
    Technologies: {', '.join(project['technologies'])}
    Features: {' '.join(project['highlights'])}
    """
    embedding = embedding_generator.embed_text(project_text)
    
    # 6. Store in Chroma
    chroma_client.add(
        collection_name="user_projects",
        documents=[project_text],
        embeddings=[embedding],
        ids=[project_id],
        metadatas={
            "user_id": user_id,
            "project_id": project_id,
            "title": project['title']
        }
    )
    
    return project_id
```

**Step 1.4: Profile Embedding**
```python
profile_text = f"""
Name: {user.name}
Skills: {', '.join(user.skills)}
Experience: {format_experience(user.experience)}
Education: {format_education(user.education)}
"""

embedding = embedding_model.encode(profile_text)

chroma_client.add(
    collection_name="user_profiles",
    documents=[profile_text],
    embeddings=[embedding],
    ids=[user.id],
    metadatas={"user_id": user.id, "type": "profile"}
)
```

### Stage 2: Job Collection & Parsing

**Step 2.1: Scrape Welcome to the Jungle**
- Use Playwright with delays (2-3 sec between requests)
- Handle pagination
- Extract: title, company, location, type, description, skills, date, URL, salary

**Step 2.2: Fetch from Adzuna API**
- API calls with retry logic
- Parse JSON response
- Stay within 5,000 calls/month limit

**Step 2.3: Normalize & Embed Jobs**
```python
job_text = f"{job.title} at {job.company}. {job.description}"
embedding = embedding_model.encode(job_text)

chroma_client.add(
    collection_name="job_descriptions",
    documents=[job_text],
    embeddings=[embedding],
    ids=[job.id],
    metadatas={
        "job_id": job.id,
        "company": job.company,
        "location": job.location,
        "job_type": job.job_type
    }
)
```

### Stage 3: Data Processing with RAG-Enhanced Ranking

**Step 3.1: Deduplication**
- Fuzzy matching (company + title, threshold: 85%)
- Prefer Welcome to the Jungle data

**Step 3.2: Filtering**
- Keyword, location, job type, date, language filters

**Step 3.3: RAG-Enhanced Ranking**
```python
def calculate_rag_match_score(user_id, job):
    # 1. Retrieve user profile
    user_profile = chroma_client.query(
        collection_name="user_profiles",
        query_embeddings=[job.embedding],
        where={"user_id": user_id},
        n_results=1
    )
    
    # 2. Semantic similarity
    semantic_similarity = cosine_similarity(
        user_profile['embeddings'][0],
        job.embedding
    )
    
    # 3. TF-IDF similarity
    tfidf_similarity = calculate_tfidf_similarity(
        user_profile['skills'],
        job.required_skills
    )
    
    # 4. Past successes boost
    past_successes = chroma_client.query(
        collection_name="successful_applications",
        query_embeddings=[job.embedding],
        where={"user_id": user_id},
        n_results=3
    )
    
    success_boost = 10 if len(past_successes['ids']) > 0 else 0
    
    # 5. Final score
    match_score = (
        semantic_similarity * 30 +
        tfidf_similarity * 30 +
        location_match * 15 +
        job_type_match * 15 +
        language_match * 5 +
        freshness * 5 +
        success_boost
    )
    
    return match_score
```

### Stage 4: RAG-Powered AI Generation

**Step 4.1: ★ Intelligent Project Selection**
```python
def select_relevant_projects(user_id, job, max_projects=3):
    # 1. Get all user projects
    all_projects = chroma_client.query(
        collection_name="user_projects",
        query_embeddings=[job.embedding],
        where={"user_id": user_id},
        n_results=10
    )
    
    if not all_projects['ids']:
        return []
    
    # 2. Calculate relevance scores
    projects_with_scores = []
    for i, project_id in enumerate(all_projects['ids'][0]):
        project = db_manager.get_project(project_id)
        
        semantic_score = all_projects['distances'][0][i]
        
        job_techs = set(job.required_skills)
        project_techs = set(project.technologies)
        tech_overlap = len(job_techs & project_techs) / len(job_techs) if job_techs else 0
        
        relevance = (semantic_score * 0.6) + (tech_overlap * 0.4)
        
        projects_with_scores.append({
            "project": project,
            "relevance_score": relevance,
            "semantic_similarity": semantic_score,
            "tech_overlap": tech_overlap,
            "matching_techs": list(job_techs & project_techs)
        })
    
    # 3. Sort and select top N
    projects_with_scores.sort(key=lambda x: x['relevance_score'], reverse=True)
    selected = projects_with_scores[:max_projects]
    
    # 4. Log selection
    for proj in projects_with_scores:
        db_manager.log_project_selection(
            project_id=proj['project'].id,
            job_id=job.id,
            relevance_score=proj['relevance_score'],
            selected=(proj in selected)
        )
    
    return selected
```

**Step 4.2: ★ RAG CV Generation with Projects**
```python
def generate_cv_with_rag_and_projects(user_id, job):
    # 1. Get CV preferences
    cv_prefs = db_manager.get_cv_preferences(user_id)
    cv_length = cv_prefs.cv_length
    include_projects = cv_prefs.include_projects
    max_projects = cv_prefs.max_projects_per_cv
    
    # 2. Retrieve context
    retriever = chroma_client.as_retriever(
        collection_name="user_profiles",
        search_kwargs={"k": 5, "filter": {"user_id": user_id}}
    )
    retrieved_docs = retriever.get_relevant_documents(job.description)
    
    # 3. Select projects
    selected_projects = []
    if include_projects:
        selected_projects = select_relevant_projects(user_id, job, max_projects)
    
    # 4. Build context
    context = build_context_from_retrieved_docs(retrieved_docs)
    
    if selected_projects:
        projects_context = "\n\n=== SELECTED PROJECTS ===\n"
        for proj_data in selected_projects:
            proj = proj_data['project']
            projects_context += f"""
Project: {proj.title}
Relevance: {proj_data['relevance_score']:.2f}
Matching Tech: {', '.join(proj_data['matching_techs'])}
Description: {proj.description}
Highlights: {' | '.join(proj.highlights[:3])}
"""
        context += projects_context
    
    # 5. Build prompt
    prompt = f"""
Generate a tailored CV using context below.

=== CONTEXT ===
{context}

=== JOB ===
Title: {job.title}
Company: {job.company}
Description: {job.description}
Skills: {', '.join(job.required_skills)}

=== REQUIREMENTS ===
- CV Length: {cv_length} page(s) MAXIMUM
- Include Projects: {include_projects}
- Projects Selected: {len(selected_projects)}
- Language: {job.language}

=== INSTRUCTIONS ===
1. PAGE LIMIT CRITICAL: Must fit {cv_length} page(s)
2. Content Prioritization for {cv_length} pages:
   {"- Concise, impactful" if cv_length == 1 else "- More detailed"}
   {"- Summary: 2-3 lines" if cv_length == 1 else "- Summary: 3-4 lines"}
   {"- Skills: 8-10 most relevant" if cv_length == 1 else "- Skills: 12-15"}
   {"- Experience: 2-3 roles, 3-4 bullets" if cv_length == 1 else "- All roles, 4-5 bullets"}
   {"- Projects: 2-3 lines each" if cv_length == 1 and selected_projects else ""}
3. Structure: Contact, Summary, Skills, Experience, Projects, Education, Languages
4. Use retrieved context for consistency
5. Tailor to job requirements

Output: Clean markdown for PDF conversion
"""
    
    # 6. Generate
    rag_chain = RetrievalQA.from_chain_type(
        llm=ChatGroq(model="llama-3.1-8b-instant", temperature=0.7),
        retriever=retriever,
        chain_type="stuff"
    )
    
    result = rag_chain({"query": prompt})
    cv_content = result['result']
    
    # 7. Validate length
    word_count = len(cv_content.split())
    estimated_pages = word_count / 400
    
    if estimated_pages > cv_length + 0.2:
        cv_content = regenerate_with_stricter_constraints(prompt, cv_length)
    
    # 8. Convert to PDF
    pdf_path = convert_markdown_to_pdf(
        cv_content,
        f"CV_{job.company}_{job.title}.pdf",
        max_pages=cv_length
    )
    
    # 9. Store
    doc_metadata = {
        "cv_length": cv_length,
        "projects_included": [p['project'].id for p in selected_projects],
        "projects_metadata": [
            {
                "project_id": str(p['project'].id),
                "title": p['project'].title,
                "relevance_score": p['relevance_score']
            }
            for p in selected_projects
        ]
    }
    
    db_manager.save_document(
        application_id=application.id,
        document_type='cv',
        file_path=pdf_path,
        metadata=doc_metadata
    )
    
    # 10. Embed
    cv_embedding = embedding_generator.embed_text(cv_content)
    chroma_client.add(
        collection_name="past_cvs",
        documents=[cv_content],
        embeddings=[cv_embedding],
        ids=[f"cv_{user_id}_{job.id}"],
        metadatas={
            "user_id": user_id,
            "job_id": job.id,
            "cv_length": cv_length,
            "projects_count": len(selected_projects)
        }
    )
    
    return cv_content, pdf_path, selected_projects, result['source_documents']
```

**Step 4.3: PDF Conversion with Page Control**
```python
def convert_markdown_to_pdf(markdown_content, output_path, max_pages=1):
    from weasyprint import HTML, CSS
    
    css = CSS(string=f'''
        @page {{
            size: A4;
            margin: 0.75in;
        }}
        body {{
            font-family: Arial, sans-serif;
            font-size: {'10pt' if max_pages == 1 else '11pt'};
            line-height: {'1.3' if max_pages == 1 else '1.4'};
        }}
        h1 {{ font-size: {'14pt' if max_pages == 1 else '16pt'}; }}
        h2 {{ font-size: {'12pt' if max_pages == 1 else '13pt'}; }}
    ''')
    
    import markdown
    html_content = markdown.markdown(markdown_content)
    
    html = HTML(string=html_content)
    pdf_doc = html.write_pdf(stylesheets=[css])
    
    with open(output_path, 'wb') as f:
        f.write(pdf_doc)
    
    # Verify page count
    from PyPDF2 import PdfReader
    reader = PdfReader(output_path)
    actual_pages = len(reader.pages)
    
    if actual_pages > max_pages:
        raise ValueError(f"CV has {actual_pages} pages, limit is {max_pages}")
    
    return output_path
```

**Step 4.4: Cover Letter Generation** (similar to CV, with RAG)

### Stage 5: Database Storage

Data flow:
1. User profile → PostgreSQL + Chroma
2. Projects → PostgreSQL + Chroma
3. Jobs → PostgreSQL + Chroma
4. Generated docs → PostgreSQL + file system + Chroma
5. High-match applications → Chroma `successful_applications`

### Stage 6: Output Generation

**CSV Export with Project Data:**
```python
def export_to_csv_with_rag(applications):
    df = pd.DataFrame([{
        'Rank': i + 1,
        'Job Title': app['job']['title'],
        'Company': app['job']['company'],
        'Match Score': app['match_score'],
        'Semantic Similarity': app['semantic_score'],
        'Projects Matched': len(app['projects']),
        'Projects Selected': len(app['selected_projects']),
        'CV Path': app['cv_path'],
        'CV Length': app['cv_length'],
        'Status': app['status']
    } for i, app in enumerate(applications)])
    
    return df
```

**Streamlit Dashboard Pages:**
1. **Profile Dashboard**: Profile creation/editing, project upload
2. **Job Search**: Search form with real-time progress
3. **Results**: Job table with RAG insights, project selection details
4. **Analytics**: Project performance, skills gap, recommendations
5. **Settings**: CV preferences, API config, RAG settings

### Stage 7: CI/CD Pipeline

**GitHub Actions Workflows:**

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
    
    steps:
    - uses: actions/checkout@v3
    - uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        playwright install chromium
    
    - name: Lint
      run: flake8 . --count --select=E9,F63,F7,F82
    
    - name: Format check
      run: black --check .
    
    - name: Type check
      run: mypy .
    
    - name: Security check
      run: bandit -r . -ll
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost/test_db
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      run: |
        pytest tests/ --cov=. --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

---

## PROJECT STRUCTURE

```
job_assistant/
│
├── .github/
│   └── workflows/
│       ├── ci.yml
│       ├── cd.yml
│       └── scheduled_scrape.yml
│
├── scrapers/
│   ├── __init__.py
│   ├── base_scraper.py
│   ├── welcome_scraper.py
│   ├── adzuna_client.py
│   └── scraper_factory.py
│
├── processing/
│   ├── __init__.py
│   ├── parser.py
│   ├── deduplicator.py
│   ├── filter_engine.py
│   ├── ranker.py
│   ├── rag_ranker.py
│   ├── project_parser.py              # ★ NEW
│   └── utils.py
│
├── ai_generation/
│   ├── __init__.py
│   ├── embeddings/
│   │   ├── __init__.py
│   │   ├── embedding_generator.py
│   │   ├── vector_store.py
│   │   └── retriever.py
│   ├── rag/
│   │   ├── __init__.py
│   │   ├── rag_pipeline.py
│   │   ├── context_builder.py
│   │   ├── profile_memory.py
│   │   ├── document_memory.py
│   │   ├── project_selector.py       # ★ NEW
│   │   └── page_optimizer.py         # ★ NEW
│   ├── base_generator.py
│   ├── cv_generator.py                # ★ UPDATED
│   ├── cover_letter_generator.py
│   ├── prompts.py
│   ├── pdf_converter.py               # ★ UPDATED
│   └── language_detector.py
│
├── database/
│   ├── __init__.py
│   ├── models.py                      # ★ UPDATED
│   ├── connection.py
│   ├── db_manager.py                  # ★ UPDATED
│   ├── vector_db_manager.py
│   └── migrations/
│       └── versions/
│           ├── 001_initial_schema.py
│           ├── 002_add_rag_tables.py
│           ├── 003_add_chroma_refs.py
│           ├── 004_add_projects_tables.py      # ★ NEW
│           └── 005_add_cv_preferences.py       # ★ NEW
│
├── output/
│   ├── __init__.py
│   ├── csv_exporter.py                # ★ UPDATED
│   ├── excel_exporter.py              # ★ UPDATED
│   └── report_generator.py
│
├── dashboard/
│   ├── __init__.py
│   ├── app.py                         # ★ UPDATED
│   ├── auth.py
│   ├── pages/
│   │   ├── 1_profile.py               # ★ UPDATED
│   │   ├── 2_search.py
│   │   ├── 3_results.py               # ★ UPDATED
│   │   ├── 4_analytics.py             # ★ UPDATED
│   │   ├── 5_settings.py              # ★ UPDATED
│   │   └── 6_rag_demo.py
│   ├── components/
│   │   ├── job_card.py
│   │   ├── filters.py
│   │   ├── charts.py
│   │   ├── rag_insights.py
│   │   ├── profile_editor.py
│   │   ├── project_uploader.py        # ★ NEW
│   │   ├── project_card.py            # ★ NEW
│   │   └── cv_preview.py              # ★ NEW
│   └── utils.py
│
├── tests/
│   ├── __init__.py
│   ├── conftest.py
│   ├── test_scrapers/
│   ├── test_processing/
│   │   └── test_project_parser.py     # ★ NEW
│   ├── test_ai_generation/
│   │   ├── test_embeddings.py
│   │   ├── test_rag_pipeline.py
│   │   ├── test_project_selector.py   # ★ NEW
│   │   ├── test_page_optimizer.py     # ★ NEW
│   │   └── test_cv_generator.py       # ★ UPDATED
│   ├── test_database/
│   │   └── test_project_operations.py # ★ NEW
│   └── test_rag/
│       └── test_project_rag.py        # ★ NEW
│
├── scripts/
│   ├── update_embeddings.py
│   ├── migrate_to_rag.py
│   ├── migrate_projects.py            # ★ NEW
│   ├── generate_report.py
│   └── send_notification.py
│
├── docs/
│   ├── setup.md
│   ├── architecture.md
│   ├── rag_guide.md
│   ├── projects_guide.md              # ★ NEW
│   └── api.md
│
├── docker/
│   ├── Dockerfile
│   ├── docker-compose.yml
│   └── .dockerignore
│
├── chroma_db/
├── generated_documents/
│   ├── cvs/
│   └── cover_letters/
├── logs/
│
├── .env.example
├── .env
├── .gitignore
├── requirements.txt
├── requirements-dev.txt
├── config.py
├── main.py
└── README.md
```

---

## DEVELOPMENT ROADMAP

### Phase 1: Project Setup (Week 1)
1. Install: VSCode, Python 3.9+, Git, PostgreSQL
2. Create GitHub repo
3. Set up virtual environment
4. Install dependencies (including RAG libraries)
5. Create project structure
6. Initialize .env file
7. Test Chroma installation

### Phase 2: Database Layer (Week 1-2)
**Part A: PostgreSQL**
1. Create schema with RAG tables
2. Implement connection management
3. Create CRUD operations
4. Set up Alembic migrations

**Part B: Chroma Vector Database**
1. Initialize Chroma client
2. Create collections
3. Implement `VectorDBManager` class
4. Test embedding and retrieval

### Phase 3: Embedding Module (Week 2)
1. Create `embedding_generator.py`
2. Create `vector_store.py` (Chroma wrapper)
3. Create `retriever.py`
4. Test embeddings
5. Benchmark performance

### Phase 3.5: ★ Project Parser Module (Week 2)
1. Create `project_parser.py`:
```python
class ProjectParser:
    def parse_readme(self, readme_content: str) -> dict:
        """Parse README.md and extract structured info"""
        html = markdown.markdown(readme_content)
        soup = BeautifulSoup(html, 'html.parser')
        
        return {
            'title': self._extract_title(soup),
            'description': self._extract_description(soup),
            'technologies': self._extract_technologies(readme_content),
            'highlights': self._extract_highlights(soup)
        }
```
2. Test with various README formats
3. Create test cases

### Phase 4: Web Scraping (Week 3)
1. Implement Welcome to the Jungle scraper
2. Implement Adzuna API client
3. **Automatically embed scraped jobs**
4. Test pipeline

### Phase 5: RAG-Enhanced Processing (Week 3-4)
1. Implement parser, deduplicator, filter
2. Implement `rag_ranker.py`
3. Test RAG ranking vs traditional
4. Create benchmarks

### Phase 6: ★ RAG-Powered AI Generation with Projects (Week 4-5)
1. Implement RAG Pipeline (`rag_pipeline.py`)
2. Implement Project Selector (`project_selector.py`)
3. Implement Page Optimizer (`page_optimizer.py`)
4. Update CV Generator with project support
5. Update Cover Letter Generator
6. Test generation quality
7. Validate page limits

### Phase 7: User Authentication & Profile with Projects (Week 5)
1. Implement authentication (`auth.py`)
2. Profile management with projects
3. Project upload UI
4. README parser integration
5. CV preferences UI
6. Test flows

### Phase 8: Streamlit Dashboard with Projects (Week 6)
1. **Page 1**: Profile + project management
2. **Page 2**: Job search
3. **Page 3**: Results with project insights
4. **Page 4**: Analytics with project performance
5. **Page 5**: Settings + CV preferences
6. Style and optimize

### Phase 9: Output & Export (Week 6)
1. CSV export with project data
2. Excel export with analytics
3. PDF reports

### Phase 10: Testing (Week 7)
1. Unit tests for all modules
2. Project-specific tests
3. RAG tests
4. Integration tests
5. Performance tests
6. Code quality checks

### Phase 11: CI/CD (Week 8)
1. Create CI workflow
2. Add RAG-specific tests
3. Create CD workflow
4. Configure monitoring

### Phase 12: Containerization (Week 8)
1. Create Dockerfile
2. Create docker-compose.yml
3. Test deployment

### Phase 13: Documentation (Week 9)
1. Update README.md
2. Create technical documentation
3. Create API documentation
4. Code polish

---

## BEST PRACTICES

### RAG Implementation
1. **Embedding Quality**
   - Use consistent model (all-MiniLM-L6-v2)
   - Re-embed on significant changes
   - Validate dimensions (384)

2. **Vector Store Management**
   - Regular backups of Chroma
   - Periodic cleanup
   - Monitor storage size
   - Use metadata filtering

3. **Retrieval Optimization**
   - Start with k=5
   - Use similarity threshold (> 0.7)
   - Cache frequent retrievals

4. **Context Building**
   - Limit context (< 4000 tokens)
   - Prioritize relevant docs
   - Format clearly

5. **Performance**
   - Batch embed when possible
   - Use async operations
   - Cache embeddings
   - Monitor latency (< 500ms)

### Project-Specific
1. **README Parsing**
   - Support multiple formats
   - Provide templates
   - Allow manual override
   - Handle errors gracefully

2. **Project Selection**
   - Semantic similarity (60%) + tech overlap (40%)
   - Log all selections for learning
   - Show clear reasoning to user

3. **Page Limit Enforcement**
   - Strict validation
   - Optimize content hierarchy
   - Test with various profiles
   - 98%+ success rate target

### Content Prioritization (1-page CV)
```python
PRIORITY_WEIGHTS_1_PAGE = {
    'contact': 1.00,
    'summary': 0.95,
    'experience': 0.90,
    'skills': 0.85,
    'projects': 0.80,
    'education': 0.75,
    'languages': 0.70
}

WORD_LIMITS_1_PAGE = {
    'contact': 30,
    'summary': 50,
    'skills': 40,
    'experience': 150,
    'projects': 100,
    'education': 30,
    'languages': 15
}
```

---

## TESTING STRATEGY

### Unit Tests
```python
# Test embedding
def test_embed_profile():
    generator = EmbeddingGenerator()
    profile = create_test_profile()
    embedding = generator.embed_profile(profile)
    assert len(embedding) == 384

# Test project parsing
def test_parse_readme():
    parser = ProjectParser()
    content = load_test_readme()
    result = parser.parse_readme(content)
    assert 'title' in result
    assert len(result['technologies']) > 0

# Test project selection
def test_select_projects():
    user_id = create_test_user_with_projects()
    job = create_test_job()
    selected = select_relevant_projects(user_id, job, max_projects=3)
    assert len(selected) <= 3
    assert selected[0]['relevance_score'] >= selected[-1]['relevance_score']

# Test page limit
def test_cv_page_limit():
    cv_content = generate_cv_with_projects(
        user_id=test_user,
        job=test_job,
        cv_length=1
    )
    pdf_path = convert_to_pdf(cv_content, max_pages=1)
    reader = PdfReader(pdf_path)
    assert len(reader.pages) == 1
```

### Integration Tests
```python
def test_full_workflow():
    # 1. Create user + profile
    user_id = create_test_user()
    profile = create_test_profile()
    
    # 2. Upload project
    project_id = handle_project_upload(
        user_id,
        project_data={'title': 'Test Project'},
        readme_file=load_test_readme()
    )
    
    # 3. Scrape jobs
    jobs = scrape_test_jobs()
    
    # 4. Rank with RAG
    ranked = RAGRanker().rank_jobs(user_id, jobs)
    
    # 5. Generate CV with projects
    cv, pdf, projects, sources = generate_cv_with_rag_and_projects(
        user_id,
        ranked[0]['job']
    )
    
    # Verify
    assert cv is not None
    assert pdf.exists()
    assert len(projects) > 0
```

---

## COMMON CHALLENGES & SOLUTIONS

### Challenge 1: Poor Retrieval Quality
**Solutions:**
- Adjust k value (3-10)
- Increase similarity threshold
- Improve text preprocessing
- Use hybrid search

### Challenge 2: Chroma Growing Large
**Solutions:**
- Document expiration
- Compression
- Archive old data

### Challenge 3: Projects Don't Fit in 1-Page
**Solutions:**
- Reduce max_projects to 2
- Use "concise" detail level
- Stricter word limits
- Warn user

### Challenge 4: Slow Embedding
**Solutions:**
- Batch operations
- Use async
- Cache embeddings

### Challenge 5: README Parsing Failures
**Solutions:**
- Provide templates
- Manual override
- Fallback to text extraction
- Support multiple formats

---

## PERFORMANCE METRICS

### Technical Targets
- Code Coverage: ≥ 75%
- Dashboard Response: < 2 seconds
- Embedding Generation: < 100ms
- Retrieval Latency: < 500ms
- RAG Pipeline: < 15 seconds
- README Parsing: < 500ms
- Project Selection: < 1 second
- CV Generation: < 15 seconds

### Functional Targets
- Jobs Scraped: 100+ per session
- Deduplication: ≥ 95%
- RAG Retrieval: ≥ 90% relevance
- Document Generation: ≥ 90% success
- Page Limit Compliance: ≥ 98%
- Project Selection: ≥ 80% relevance

---

## DEPLOYMENT CHECKLIST

### Pre-Deployment
- [ ] All tests passing
- [ ] Code quality checks passing
- [ ] Security scan clean
- [ ] No secrets in code
- [ ] .env.example updated
- [ ] README updated
- [ ] Dependencies locked
- [ ] Chroma initialized
- [ ] Embedding model downloaded

### RAG-Specific
- [ ] Test embeddings in production
- [ ] Verify Chroma persistence
- [ ] Test vector store backup
- [ ] Validate retrieval performance
- [ ] Test with concurrent users

### Deployment
- [ ] PostgreSQL hosted (Supabase)
- [ ] Chroma persistence configured
- [ ] Environment variables set
- [ ] Deploy to Streamlit Cloud
- [ ] Monitoring configured (Sentry)
- [ ] CI/CD active
- [ ] Backups configured

### Post-Deployment
- [ ] Test all features
- [ ] Monitor logs
- [ ] Check performance
- [ ] Verify API limits

---

## CRITICAL REMINDERS

### For Implementation
1. **NEVER use localStorage/sessionStorage in Streamlit artifacts** - not supported
2. **Always use try-catch for storage operations** - can fail
3. **Validate embedding dimensions** - must be 384
4. **Test page limits rigorously** - 98%+ success rate
5. **Batch operations when possible** - performance
6. **Cache frequently accessed data** - reduce latency
7. **Monitor API rate limits** - Groq: 30 req/min
8. **Backup Chroma regularly** - critical data

### For RAG Quality
1. **k=5 is starting point** - adjust based on results
2. **Similarity threshold > 0.7** - filter low quality
3. **Context < 4000 tokens** - LLM limit
4. **Store all generated docs** - for learning
5. **Track consistency metrics** - ensure quality

### For Projects Feature
1. **Projects are optional** - must work without them
2. **README parsing can fail** - have fallbacks
3. **Page limits are strict** - validate output
4. **Selection reasoning matters** - log everything
5. **Test with 0, 1, 5, 10 projects** - edge cases

---

## GLOSSARY

**RAG**: Retrieval-Augmented Generation - retrieves context before generating
**Vector Database**: Database for storing/searching embeddings
**Embedding**: Numerical representation of text (384 dimensions)
**Semantic Similarity**: Meaning-based similarity (not just keywords)
**Cosine Similarity**: Similarity measure between vectors (-1 to 1)
**k-NN**: k-Nearest Neighbors retrieval
**Context Window**: Max text for LLM (8K tokens for Llama 3.1)
**Chroma Collection**: Group of documents in Chroma
**Project Relevance**: Semantic (60%) + tech overlap (40%)
**Page Optimizer**: Component that fits content in page limit

---

## ENVIRONMENT VARIABLES

```bash
# .env.example

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/jobassistant
CHROMA_PERSIST_DIR=./chroma_db

# APIs
GROQ_API_KEY=your_groq_key_here
ADZUNA_APP_ID=your_adzuna_app_id
ADZUNA_API_KEY=your_adzuna_key

# Monitoring
SENTRY_DSN=your_sentry_dsn

# App Config
DEBUG=False
LOG_LEVEL=INFO
```

---

This specification is complete and ready for implementation. Reference specific sections as needed during development.